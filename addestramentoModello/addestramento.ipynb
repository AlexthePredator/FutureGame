{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c29b3e87",
   "metadata": {},
   "source": [
    "# Librerie\n",
    "caricamento librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c04bc75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b0b3f3",
   "metadata": {},
   "source": [
    "### Carica DataSet\n",
    "games_march2025_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b5168750",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"DataSet/games_march2025_cleaned.csv\")\n",
    "\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b071e6",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c813b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adb9c47",
   "metadata": {},
   "source": [
    "score_rank troppi null \n",
    "metacritic_url non significativo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabfe770",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()\n",
    "#statistiche del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f996b5",
   "metadata": {},
   "source": [
    "* Eliminiamo colonna 'appid', id interno al dataset di steam, non utile alla prediizione\n",
    "* Eliminiamo colonna della descrizione e raccomandazioni, sono testuali e non utili alla predizione.\n",
    "    * 'detailed_description' , 'about_the_game', 'short_description', 'reviews'\n",
    "* Eliminiamo url dell'immagine del gioco\n",
    "    * 'header_image'\n",
    "* Eliminiamo colonne testuali per supporto clienti e url vari\n",
    "    * 'website', 'support_url', 'support_email', 'metacritic_url', 'notes'\n",
    "* Eliminiamo colonne per trailer video e foto del gioco\n",
    "    * 'movies' , 'screenshots'\n",
    "* Elimino i punteggi che hanno troppi valori null\n",
    "    * 'user_score', 'score_rank'\n",
    "* Elimino le liste delle lingue supportate e dei pacchetti che non danno utilità\n",
    "    * 'supported_languages', 'full_audio_languages', 'packages'\n",
    "* Elimino numero di dlc e sconti che non danno informazioni o con molti valori nulli\n",
    "    * 'discount', 'dlc_count', 'tags'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bbe0cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop elimina colonna, per eliminare più colonne contemporaneamente [ , ]\n",
    "dataset.drop('appid', axis=1, inplace=True)\n",
    "dataset.drop('name', axis=1, inplace=True)\n",
    "dataset.drop(['detailed_description','about_the_game', 'short_description', 'reviews', 'header_image'], axis=1, inplace=True)\n",
    "dataset.drop(['website', 'support_url', 'support_email', 'metacritic_url', 'notes'], axis=1, inplace=True)\n",
    "dataset.drop(['screenshots','movies'], axis=1, inplace=True)\n",
    "dataset.drop(['user_score', 'score_rank', 'metacritic_score'], axis=1, inplace=True)\n",
    "dataset.drop(['supported_languages', 'full_audio_languages', 'packages', 'tags'], axis=1, inplace=True)\n",
    "dataset.drop(['discount', 'dlc_count'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae5ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.describe(include=['O'])\n",
    "dataset['user_score'].value_counts()\n",
    "dataset['metacritic_score'].value_counts()\n",
    "dataset['score_rank'].value_counts()\n",
    "dataset['positive'].value_counts()\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed7809",
   "metadata": {},
   "source": [
    "### Salvo su nuovo dataset da poter usare comodamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4943de8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvo dataset modificato in uno nuovo per poter ripartire da lì\n",
    "dataset.to_csv('DataSet/games1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aab6c0",
   "metadata": {},
   "source": [
    "##### Salvo le modifiche in un nuovo dataset per poterlo utilizzare\n",
    "* Inizializzare da QUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2bf9cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUOVO DATASET CON MODIFICHE\n",
    "\n",
    "ds = pd.read_csv(\"DataSet/games1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b801b6d",
   "metadata": {},
   "source": [
    "* Creo nuova variabile che include differenza tra positivi e negativi\n",
    "    * elimino le due colonne di positivi e negativi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e62fe23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['absolute_score'] = ds['positive'] - ds['negative']\n",
    "\n",
    "ds.drop(['positive', 'negative'], axis=1, inplace=True)\n",
    "ds.to_csv('DataSet/games1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec38423",
   "metadata": {},
   "source": [
    "### Taglio dataset sugli ultimi 10 anni:\n",
    "* taglio tutti i giochi usciti prima del 2015\n",
    "    * dataset arriva fino al 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b492b0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Righe rimanenti: 86847\n"
     ]
    }
   ],
   "source": [
    "ds['release_date'] = pd.to_datetime(ds['release_date'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "cutoff_date = pd.Timestamp('2015-01-01')\n",
    "\n",
    "# Filtriamo solo le righe dal 2015 in poi\n",
    "ds = ds[ds['release_date'] >= cutoff_date]\n",
    "\n",
    "print(f\"Righe rimanenti: {len(ds)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7058b14",
   "metadata": {},
   "source": [
    "### Variabile Target num giocatori che hanno acquistato il gioco\n",
    "* per decretare successo del gioco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "63f5c535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "estimated_owners\n",
       "0            67288\n",
       "20000         9144\n",
       "50000         4130\n",
       "100000        2586\n",
       "200000        2022\n",
       "500000         833\n",
       "1000000        441\n",
       "2000000        263\n",
       "5000000         78\n",
       "10000000        34\n",
       "20000000        18\n",
       "50000000        10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['estimated_owners'] = ds['estimated_owners'].apply(lambda x: x.split('-')[0])\n",
    "ds['estimated_owners'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180ae5d7",
   "metadata": {},
   "source": [
    "### Raggruppamento\n",
    "* Faccio raggruppamento\n",
    "    * raggruppo per 4 valori quindi 4 intervalli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4d59ccf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success_class\n",
      "insuccesso        67288\n",
      "basso_successo    13274\n",
      "medio_successo     5441\n",
      "alto_successo       844\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# dopo il taglio era diventato stringhe\n",
    "ds['estimated_owners'] = pd.to_numeric(ds['estimated_owners'], errors='coerce')\n",
    "\n",
    "intervalli = [0, 5000, 50000, 500000, float('inf')]\n",
    "etichette = ['insuccesso', 'basso_successo', 'medio_successo', 'alto_successo']\n",
    "\n",
    "ds['success_class'] = pd.cut(ds['estimated_owners'], bins=intervalli, labels=etichette, include_lowest=True)\n",
    "\n",
    "print(ds['success_class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d7dae6",
   "metadata": {},
   "source": [
    "### Eseguo Undersampling\n",
    "* Dataset troppo sbilanciato meglio eseguire undersampling e portare tutte le classi di successo allo stesso valore della più piccola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "76109e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione originale:\n",
      "success_class\n",
      "insuccesso        67288\n",
      "basso_successo    13274\n",
      "medio_successo     5441\n",
      "alto_successo       844\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribuzione dopo undersampling:\n",
      "success_class\n",
      "insuccesso        844\n",
      "basso_successo    844\n",
      "medio_successo    844\n",
      "alto_successo     844\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# X = tutte le colonne tranne la target\n",
    "X = ds.drop(columns=['success_class'])\n",
    "\n",
    "# y = la target\n",
    "y = ds['success_class']\n",
    "\n",
    "undersampler = RandomUnderSampler( sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Eseguo Undersampling\n",
    "X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
    "\n",
    "# Ricombino in un nuovo DataFrame\n",
    "ds = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "\n",
    "# elimino colonna 'estimated_owners' non serve più - sostituita da 'success_class'\n",
    "ds.drop('estimated_owners', axis=1, inplace=True)\n",
    "\n",
    "print(\"Distribuzione originale:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "print(\"\\nDistribuzione dopo undersampling:\")\n",
    "print(y_resampled.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac48550",
   "metadata": {},
   "source": [
    "### Elimino colonne che hanno troppe variabili nulle e che vanno a squilibrare il dataset\n",
    "* 'pct_pos_total', 'pct_pos_recent', 'num_reviews_total', 'num_reviews_recent', 'peak_ccu'\n",
    "    * hanno varibile nulla => 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a7a9782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_reviews_recent\n",
      "-1       2154\n",
      " 10        25\n",
      " 11        23\n",
      " 12        23\n",
      " 13        23\n",
      "         ... \n",
      " 854        1\n",
      " 108        1\n",
      " 889        1\n",
      " 139        1\n",
      " 1101       1\n",
      "Name: count, Length: 577, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(ds['num_reviews_recent'].value_counts())\n",
    "ds.drop(['pct_pos_total', 'pct_pos_recent', 'num_reviews_total', 'num_reviews_recent', 'peak_ccu'] , axis=1, inplace=True)\n",
    "\n",
    "ds.to_csv('DataSet/games1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff263d9",
   "metadata": {},
   "source": [
    "# Lettura da games1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92e84b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\"DataSet/games1.csv\")\n",
    "#print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ce91f",
   "metadata": {},
   "source": [
    "# Encoding delle variabili\n",
    "* Ci sono colonne che contengono liste di variabili.\n",
    "    * Eseguo Encoding su 'genres' e 'categories'\n",
    "\n",
    "### Encoding di genres e categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "451476df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Per 'genres'\n",
    "# Converto le stringhe in liste\n",
    "ds['genres'] = ds['genres'].apply(ast.literal_eval)\n",
    "\n",
    "# mlb classe contiene ordine dei generi\n",
    "mlb = MultiLabelBinarizer()\n",
    "genres_encoded = mlb.fit_transform(ds['genres'])\n",
    "\n",
    "# Salvo in una colonna come array\n",
    "ds['genres_vector'] = list(genres_encoded)\n",
    "\n",
    "# Per 'categories'\n",
    "ds['categories'] = ds['categories'].apply(ast.literal_eval)\n",
    "\n",
    "mlb_categories = MultiLabelBinarizer()\n",
    "categories_encoded = mlb_categories.fit_transform(ds['categories'])\n",
    "\n",
    "ds['categories_vector'] = list(categories_encoded)\n",
    "\n",
    "# esempio\n",
    "print(ds['genres_vector'].iloc[0])\n",
    "print(ds['categories_vector'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72fc033",
   "metadata": {},
   "source": [
    "### Encoding di developers e publishers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d1f5f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "ds['developers'] = ds['developers'].apply(ast.literal_eval)\n",
    "mlb_developers = MultiLabelBinarizer()\n",
    "developers_encoded = mlb_developers.fit_transform(ds['developers'])\n",
    "ds['developers_vector'] = list(developers_encoded)\n",
    "\n",
    "\n",
    "ds['publishers'] = ds['publishers'].apply(ast.literal_eval)\n",
    "mlb_publishers = MultiLabelBinarizer()\n",
    "publishers_encoded = mlb_publishers.fit_transform(ds['publishers'])\n",
    "ds['publishers_vector'] = list(publishers_encoded)\n",
    "\n",
    "# esempio\n",
    "print(ds['developers_vector'].iloc[0])\n",
    "print(ds['publishers_vector'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e444df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds.drop(['genres', 'categories'], axis=1, inplace=True)\n",
    "ds.drop(['developers', 'publishers'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e11c5e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_csv('DataSet/games1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdfc7fc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2022-06-23'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Utilizziamo il training set per addestrare il modello\u001b[39;00m\n\u001b[32m     12\u001b[39m model = DecisionTreeClassifier()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1024\u001b[39m, in \u001b[36mDecisionTreeClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m    993\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, check_input=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    995\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[32m    996\u001b[39m \n\u001b[32m    997\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1021\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m   1022\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1030\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:252\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    248\u001b[39m check_X_params = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    249\u001b[39m     dtype=DTYPE, accept_sparse=\u001b[33m\"\u001b[39m\u001b[33mcsc\u001b[39m\u001b[33m\"\u001b[39m, ensure_all_finite=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    250\u001b[39m )\n\u001b[32m    251\u001b[39m check_y_params = \u001b[38;5;28mdict\u001b[39m(ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_separately\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_y_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m missing_values_in_feature_mask = (\n\u001b[32m    257\u001b[39m     \u001b[38;5;28mself\u001b[39m._compute_missing_values_in_feature_mask(X)\n\u001b[32m    258\u001b[39m )\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2956\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2954\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mestimator\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_X_params:\n\u001b[32m   2955\u001b[39m     check_X_params = {**default_check_params, **check_X_params}\n\u001b[32m-> \u001b[39m\u001b[32m2956\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2957\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mestimator\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_y_params:\n\u001b[32m   2958\u001b[39m     check_y_params = {**default_check_params, **check_y_params}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:973\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pandas_requires_conversion:\n\u001b[32m    969\u001b[39m     \u001b[38;5;66;03m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[32m    970\u001b[39m     \u001b[38;5;66;03m# nans\u001b[39;00m\n\u001b[32m    971\u001b[39m     \u001b[38;5;66;03m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[32m    972\u001b[39m     new_dtype = dtype_orig \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m     array = \u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m     \u001b[38;5;66;03m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[32m    975\u001b[39m     dtype = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:6662\u001b[39m, in \u001b[36mNDFrame.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m   6656\u001b[39m     results = [\n\u001b[32m   6657\u001b[39m         ser.astype(dtype, copy=copy, errors=errors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items()\n\u001b[32m   6658\u001b[39m     ]\n\u001b[32m   6660\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6661\u001b[39m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6662\u001b[39m     new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6663\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes)\n\u001b[32m   6664\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mastype\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:430\u001b[39m, in \u001b[36mBaseBlockManager.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[32m    428\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mastype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:784\u001b[39m, in \u001b[36mBlock.astype\u001b[39m\u001b[34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[39m\n\u001b[32m    781\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan not squeeze with more than one column.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    782\u001b[39m     values = values[\u001b[32m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m784\u001b[39m new_values = \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    786\u001b[39m new_values = maybe_coerce_values(new_values)\n\u001b[32m    788\u001b[39m refs = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001b[39m, in \u001b[36mastype_array_safe\u001b[39m\u001b[34m(values, dtype, copy, errors)\u001b[39m\n\u001b[32m    234\u001b[39m     dtype = dtype.numpy_dtype\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     new_values = \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001b[39m, in \u001b[36mastype_array\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    179\u001b[39m     values = values.astype(dtype, copy=copy)\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     values = \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np.dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alex\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:133\u001b[39m, in \u001b[36m_astype_nansafe\u001b[39m\u001b[34m(arr, dtype, copy, skipna)\u001b[39m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype == \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m arr.astype(dtype, copy=copy)\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: '2022-06-23'"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "# Separiamo i nostri dati in dati di training e dati di test\n",
    "# Specifichiamo la proporzione fra i due con test_size; in questo caso abbiamo impostato il training set al 90% e il test set al 10%\n",
    "X = ds.drop(columns=['success_class'])  # tutte le colonne tranne il target\n",
    "y = ds['success_class']  \n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Utilizziamo il training set per addestrare il modello\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
